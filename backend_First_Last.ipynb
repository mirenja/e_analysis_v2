{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Backend: Germany Power Generation – Clean, Step-by-Step Solution\n",
        "\n",
        "**Scope:** Hourly electricity generation by fuel for Germany (Oct–Dec 2022), cleaned & aggregated.  \n",
        "**Deliverables:** Cleaned hourly table (wide), daily & monthly aggregates, renewable vs non-renewable tables, and figures.  \n",
        "**Run order:** Execute cells from top to bottom. If the API is unavailable, the notebook will look for a local file in `./data/`.\n",
        "\n",
        "> **Rename this notebook to** `backend_<first_name>_<last_name>.ipynb` before submission.\n",
        "THIS WAS A GUIDE B CHAT GPT PLEASE dont USE THIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 0. Setup & Imports ---\n",
        "# If you don't have some packages, install them with:\n",
        "# %pip install pandas numpy matplotlib requests python-dotenv pyarrow\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "\n",
        "DATA_DIR = Path(\"./data\")\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Optional: load API credentials from a .env file placed next to this notebook\n",
        "# .env example:\n",
        "#   BASE_URL=https://<provided-api-endpoint>/query\n",
        "#   API_KEY=xxxxxxxxxxxxxxxxxxxxxxxx\n",
        "try:\n",
        "    from dotenv import load_dotenv  # type: ignore\n",
        "    load_dotenv()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "BASE_URL = os.getenv(\"BASE_URL\", \"\").strip()  # provided by organizer\n",
        "API_KEY  = os.getenv(\"API_KEY\", \"\").strip()\n",
        "\n",
        "DATASET  = \"task_generation_h\"\n",
        "DATE_FROM = \"2022-10-01\"\n",
        "DATE_TO   = \"2022-12-31\"\n",
        "\n",
        "# Local cache filenames (used when API is down or to avoid re-downloading)\n",
        "RAW_JSON_PATH = DATA_DIR / \"raw_generation.json\"\n",
        "CLEAN_HOURLY_WIDE_CSV = DATA_DIR / \"germany_hourly_by_fuel_cleaned.csv\"\n",
        "DAILY_CSV = DATA_DIR / \"germany_daily_by_fuel_MWh.csv\"\n",
        "MONTHLY_CSV = DATA_DIR / \"germany_monthly_by_fuel_MWh.csv\"\n",
        "REN_HOURLY_CSV = DATA_DIR / \"germany_hourly_renewable_nonrenewable.csv\"\n",
        "REN_DAILY_CSV  = DATA_DIR / \"germany_daily_renewable_nonrenewable.csv\"\n",
        "REN_MONTHLY_CSV= DATA_DIR / \"germany_monthly_renewable_nonrenewable.csv\"\n",
        "LONG_FORMAT_CSV= DATA_DIR / \"germany_hourly_long.csv\"\n",
        "\n",
        "# Figures\n",
        "FIG_STACKED_DAILY = DATA_DIR / \"figure_stacked_area_daily_GWh.png\"\n",
        "FIG_REN_BAR = DATA_DIR / \"figure_renewable_vs_nonrenewable_Oct_Dec_GWh.png\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 1. Acquire Data ---\n",
        "# Strategy:\n",
        "# 1) Try to load from local cache (./data/raw_generation.json).\n",
        "# 2) If missing, try API (BASE_URL, API_KEY) – saves cache if successful.\n",
        "# 3) If API unavailable, look for an already-downloaded file under ./data/:\n",
        "#       - 'task_generation_h.csv'   (preferred if you exported from the provided link)\n",
        "#       - 'task_generation_h.parquet'\n",
        "#    If none exist, **download the dataset manually** from the shared link and place it in ./data/,\n",
        "#    then re-run this cell.\n",
        "\n",
        "def load_from_api():\n",
        "    if not BASE_URL or not API_KEY:\n",
        "        return None\n",
        "    import requests\n",
        "    headers = {\"API-Key\": API_KEY}\n",
        "    params = {\"dataset\": DATASET, \"from\": DATE_FROM, \"to\": DATE_TO}\n",
        "    r = requests.get(BASE_URL, headers=headers, params=params, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    payload = r.json()\n",
        "    # Expecting keys: \"columns\" and \"data\"\n",
        "    df = pd.DataFrame(payload.get(\"data\", []))\n",
        "    if \"columns\" in payload and df.shape[1] == len(payload[\"columns\"]):\n",
        "        df.columns = payload[\"columns\"]\n",
        "    return df\n",
        "\n",
        "def load_from_local_file():\n",
        "    # Try Parquet or CSV that the candidate has placed under ./data\n",
        "    pq = DATA_DIR / \"task_generation_h.parquet\"\n",
        "    csv = DATA_DIR / \"task_generation_h.csv\"\n",
        "    if pq.exists():\n",
        "        return pd.read_parquet(pq)\n",
        "    if csv.exists():\n",
        "        return pd.read_csv(csv)\n",
        "    return None\n",
        "\n",
        "def load_data():\n",
        "    # 1) local cache JSON\n",
        "    if RAW_JSON_PATH.exists():\n",
        "        try:\n",
        "            obj = json.loads(RAW_JSON_PATH.read_text())\n",
        "            df = pd.DataFrame(obj.get(\"data\", []))\n",
        "            if \"columns\" in obj and df.shape[1] == len(obj[\"columns\"]):\n",
        "                df.columns = obj[\"columns\"]\n",
        "            if not df.empty:\n",
        "                print(\"Loaded cached API JSON:\", RAW_JSON_PATH)\n",
        "                return df\n",
        "        except Exception as e:\n",
        "            print(\"Cached JSON exists but could not be read:\", e)\n",
        "\n",
        "    # 2) API\n",
        "    try:\n",
        "        df_api = load_from_api()\n",
        "        if df_api is not None and not df_api.empty:\n",
        "            print(\"Loaded fresh data from API.\")\n",
        "            # Save cache\n",
        "            cache_obj = {\"columns\": list(df_api.columns), \"data\": df_api.values.tolist()}\n",
        "            RAW_JSON_PATH.write_text(json.dumps(cache_obj, indent=2))\n",
        "            print(\"Saved API cache to\", RAW_JSON_PATH)\n",
        "            return df_api\n",
        "    except Exception as e:\n",
        "        print(\"API fetch failed:\", repr(e))\n",
        "\n",
        "    # 3) Local manual download\n",
        "    df_local = load_from_local_file()\n",
        "    if df_local is not None and not df_local.empty:\n",
        "        print(\"Loaded local file from ./data\")\n",
        "        return df_local\n",
        "\n",
        "    raise FileNotFoundError(\n",
        "        \"No data available. Please download the dataset from the provided link \"\n",
        "        \"and place it as './data/task_generation_h.csv' (or .parquet), then re-run.\"\n",
        "    )\n",
        "\n",
        "df_raw = load_data()\n",
        "df_raw.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 2. Filter for Germany & Pivot ---\n",
        "# The raw schema is expected to contain: ['date_id', 'region', 'generation', 'value']\n",
        "# We'll keep 'Germany', set 'date_id' as DatetimeIndex (hourly), and pivot to wide columns per fuel.\n",
        "\n",
        "expected_cols = {\"date_id\",\"region\",\"generation\",\"value\"}\n",
        "missing = expected_cols - set(df_raw.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"Input missing expected columns: {missing}\")\n",
        "\n",
        "df_raw[\"date_id\"] = pd.to_datetime(df_raw[\"date_id\"])\n",
        "df_de = df_raw[df_raw[\"region\"] == \"Germany\"].copy()\n",
        "df_de = df_de.sort_values(\"date_id\")\n",
        "\n",
        "# Pivot: wide by generation (fuel type)\n",
        "dfw = df_de.pivot_table(\n",
        "    index=\"date_id\",\n",
        "    columns=\"generation\",\n",
        "    values=\"value\",\n",
        "    aggfunc=\"sum\"  # in case there are duplicate rows, sum within the hour\n",
        ").sort_index()\n",
        "\n",
        "# (Optional) ensure hourly frequency if you want a full range with gaps visible\n",
        "full_range = pd.date_range(dfw.index.min(), dfw.index.max(), freq=\"H\")\n",
        "dfw = dfw.reindex(full_range)  # keep NaN where data is missing; we will impute later\n",
        "dfw.index.name = \"date_id\"\n",
        "\n",
        "print(\"Wide hourly table (Germany) – shape:\", dfw.shape)\n",
        "dfw.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 3. Replace Deliberately Large Numbers (Outliers) ---\n",
        "# We treat \"implausibly large spikes\" using a robust rule:\n",
        "#   Flag values > Q3 + 1.5*IQR or < Q1 - 1.5*IQR (per-fuel).\n",
        "# Replacement strategy:\n",
        "#   Replace each outlier by the median for the *same month* and *same hour-of-day* for that fuel.\n",
        "#   If that median is unavailable, fall back to the overall median for that fuel.\n",
        "\n",
        "def month_hour_medians(s: pd.Series) -> pd.Series:\n",
        "    # Returns a Series aligned to s.index with month-hour median values\n",
        "    month = s.index.to_period(\"M\")\n",
        "    hour  = s.index.hour\n",
        "    key = pd.MultiIndex.from_arrays([month, hour], names=[\"month\",\"hour\"])\n",
        "    med = s.groupby(key).transform(\"median\")\n",
        "    # 'med' is aligned to s by construction\n",
        "    return med\n",
        "\n",
        "def replace_outliers_iqr(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for col in df.columns:\n",
        "        s = df[col]\n",
        "        q1, q3 = s.quantile(0.25), s.quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        if pd.isna(iqr) or iqr == 0:\n",
        "            # Not enough variation; skip\n",
        "            continue\n",
        "        fence_low  = q1 - 1.5 * iqr\n",
        "        fence_high = q3 + 1.5 * iqr\n",
        "        mask = (s < fence_low) | (s > fence_high)\n",
        "        if mask.any():\n",
        "            mh_med = month_hour_medians(s)\n",
        "            fallback = s.median()\n",
        "            repl = mh_med.where(~mh_med.isna(), other=fallback)\n",
        "            df.loc[mask, col] = repl[mask]\n",
        "    return df\n",
        "\n",
        "dfw_no_outliers = replace_outliers_iqr(dfw)\n",
        "dfw_no_outliers.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 4. Impute Missing Data (Intentional Gaps) ---\n",
        "# Two-step imputation per fuel:\n",
        "#   1) Time-based linear interpolation (works well for hourly series).\n",
        "#   2) Fill any remaining gaps with month-hour median; fallback to overall median.\n",
        "\n",
        "def impute_missing(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    # Step 1: linear interpolation over time\n",
        "    df = df.interpolate(method=\"time\", limit_direction=\"both\")\n",
        "\n",
        "    # Step 2: month-hour median per column\n",
        "    for col in df.columns:\n",
        "        s = df[col]\n",
        "        still_na = s.isna()\n",
        "        if still_na.any():\n",
        "            mh_med = month_hour_medians(s)\n",
        "            s.loc[still_na] = mh_med.loc[still_na]\n",
        "            # final fallback\n",
        "            still_na = s.isna()\n",
        "            if still_na.any():\n",
        "                s.loc[still_na] = s.median()\n",
        "        df[col] = s\n",
        "    return df\n",
        "\n",
        "dfw_clean = impute_missing(dfw_no_outliers)\n",
        "print(\"Any remaining NaNs?\", dfw_clean.isna().sum().sum())\n",
        "dfw_clean.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 5. Aggregate Daily & Monthly by Fuel ---\n",
        "# IMPORTANT: The raw is hourly MWh. Aggregation should be **sum**, not mean.\n",
        "# We compute daily and monthly totals per fuel (MWh).\n",
        "\n",
        "df_daily = dfw_clean.resample(\"D\").sum()\n",
        "df_monthly = dfw_clean.resample(\"M\").sum()\n",
        "\n",
        "print(\"Daily shape:\", df_daily.shape, \"Monthly shape:\", df_monthly.shape)\n",
        "df_daily.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 6. BONUS: Renewable vs Non-Renewable ---\n",
        "# Map each fuel type to one of two categories, then compute hourly, daily, monthly\n",
        "# totals and percentages.\n",
        "\n",
        "renewable_sources = [\n",
        "    \"Biomass\", \"Dam Hydro\", \"Geothermal\", \"Other renewables\",\n",
        "    \"Pumped storage generation\", \"Run-of-River Hydro\",\n",
        "    \"Solar\", \"Wind offshore\", \"Wind onshore\"\n",
        "]\n",
        "\n",
        "non_renewable_sources = [\n",
        "    \"Hard Coal\", \"Lignite\", \"Natural Gas\", \"Non-renewable waste\",\n",
        "    \"Nuclear\", \"Oil\", \"Other fossil fuel\"\n",
        "]\n",
        "\n",
        "# Keep only columns present in the dataset\n",
        "ren_cols = [c for c in renewable_sources if c in dfw_clean.columns]\n",
        "nren_cols = [c for c in non_renewable_sources if c in dfw_clean.columns]\n",
        "\n",
        "hourly_ren = pd.DataFrame({\n",
        "    \"renewable_MWh\": dfw_clean[ren_cols].sum(axis=1) if ren_cols else 0.0,\n",
        "    \"non_renewable_MWh\": dfw_clean[nren_cols].sum(axis=1) if nren_cols else 0.0,\n",
        "})\n",
        "hourly_ren[\"total_MWh\"] = hourly_ren.sum(axis=1)\n",
        "hourly_ren[\"renewable_%\"] = np.where(hourly_ren[\"total_MWh\"]>0, 100*hourly_ren[\"renewable_MWh\"]/hourly_ren[\"total_MWh\"], np.nan)\n",
        "hourly_ren[\"non_renewable_%\"] = np.where(hourly_ren[\"total_MWh\"]>0, 100*hourly_ren[\"non_renewable_MWh\"]/hourly_ren[\"total_MWh\"], np.nan)\n",
        "\n",
        "daily_ren = hourly_ren.resample(\"D\").sum()\n",
        "daily_ren[\"renewable_%\"] = np.where(daily_ren[\"total_MWh\"]>0, 100*daily_ren[\"renewable_MWh\"]/daily_ren[\"total_MWh\"], np.nan)\n",
        "daily_ren[\"non_renewable_%\"] = np.where(daily_ren[\"total_MWh\"]>0, 100*daily_ren[\"non_renewable_MWh\"]/daily_ren[\"total_MWh\"], np.nan)\n",
        "\n",
        "monthly_ren = hourly_ren.resample(\"M\").sum()\n",
        "monthly_ren[\"renewable_%\"] = np.where(monthly_ren[\"total_MWh\"]>0, 100*monthly_ren[\"renewable_MWh\"]/monthly_ren[\"total_MWh\"], np.nan)\n",
        "monthly_ren[\"non_renewable_%\"] = np.where(monthly_ren[\"total_MWh\"]>0, 100*monthly_ren[\"non_renewable_MWh\"]/monthly_ren[\"total_MWh\"], np.nan)\n",
        "\n",
        "hourly_ren.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 7. BONUS: Back to Long Format ---\n",
        "# Convert the **cleaned hourly wide** table back to the original long schema:\n",
        "# ['date_id','region','generation','value'] for Germany only.\n",
        "\n",
        "df_long = (\n",
        "    dfw_clean\n",
        "      .reset_index()\n",
        "      .melt(id_vars=\"date_id\", var_name=\"generation\", value_name=\"value\")\n",
        "      .assign(region=\"Germany\")\n",
        "      .loc[:, [\"date_id\",\"region\",\"generation\",\"value\"]]\n",
        "      .sort_values([\"date_id\",\"generation\"])\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "df_long.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 8. Plots (Matplotlib) ---\n",
        "# 1) Stacked area of **daily** totals by fuel (GWh).\n",
        "# 2) (Bonus) Bar chart comparing Renewable vs Non-Renewable in Oct and Dec 2022 (GWh).\n",
        "\n",
        "# 1) Stacked area (daily by fuel)\n",
        "daily_gwh = df_daily / 1000.0  # MWh -> GWh\n",
        "ax = daily_gwh.plot.area()\n",
        "ax.set_title(\"Germany – Daily Power Generation by Fuel (GWh)\")\n",
        "ax.set_xlabel(\"Date\")\n",
        "ax.set_ylabel(\"GWh\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_STACKED_DAILY, dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# 2) Renewable vs Non-Renewable for Oct & Dec 2022\n",
        "oct_mask = (daily_ren.index >= \"2022-10-01\") & (daily_ren.index <= \"2022-10-31\")\n",
        "dec_mask = (daily_ren.index >= \"2022-12-01\") & (daily_ren.index <= \"2022-12-31\")\n",
        "\n",
        "oct_totals = daily_ren.loc[oct_mask, [\"renewable_MWh\",\"non_renewable_MWh\"]].sum()/1000.0\n",
        "dec_totals = daily_ren.loc[dec_mask, [\"renewable_MWh\",\"non_renewable_MWh\"]].sum()/1000.0\n",
        "\n",
        "fig = plt.figure()\n",
        "x = np.arange(2)\n",
        "width = 0.35\n",
        "plt.bar(x - width/2, [oct_totals[\"renewable_MWh\"], dec_totals[\"renewable_MWh\"]], width, label=\"Renewable\")\n",
        "plt.bar(x + width/2, [oct_totals[\"non_renewable_MWh\"], dec_totals[\"non_renewable_MWh\"]], width, label=\"Non-Renewable\")\n",
        "plt.xticks(x, [\"Oct 2022\", \"Dec 2022\"])\n",
        "plt.ylabel(\"GWh\")\n",
        "plt.title(\"Germany – Renewable vs Non-Renewable (Oct & Dec 2022, GWh)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_REN_BAR, dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 9. Save Outputs ---\n",
        "dfw_clean.to_csv(CLEAN_HOURLY_WIDE_CSV, index=True)\n",
        "df_daily.to_csv(DAILY_CSV, index=True)\n",
        "df_monthly.to_csv(MONTHLY_CSV, index=True)\n",
        "\n",
        "hourly_ren.to_csv(REN_HOURLY_CSV, index=True)\n",
        "daily_ren.to_csv(REN_DAILY_CSV, index=True)\n",
        "monthly_ren.to_csv(REN_MONTHLY_CSV, index=True)\n",
        "\n",
        "df_long.to_csv(LONG_FORMAT_CSV, index=False)\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(\" -\", CLEAN_HOURLY_WIDE_CSV)\n",
        "print(\" -\", DAILY_CSV)\n",
        "print(\" -\", MONTHLY_CSV)\n",
        "print(\" -\", REN_HOURLY_CSV)\n",
        "print(\" -\", REN_DAILY_CSV)\n",
        "print(\" -\", REN_MONTHLY_CSV)\n",
        "print(\" -\", LONG_FORMAT_CSV)\n",
        "print(\" -\", FIG_STACKED_DAILY)\n",
        "print(\" -\", FIG_REN_BAR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# --- 10. Notes & Assumptions (for reviewers) ---\n",
        "# • Aggregation uses **sum** (MWh → daily/monthly totals). The original draft used mean;\n",
        "#   totals are the typical requirement for energy generation.\n",
        "# • Outliers: substituted with the month × hour-of-day median per fuel – a \"reasonable\" value\n",
        "#   that respects diurnal and seasonal patterns without using future information.\n",
        "# • Missing values: linear time interpolation first; remaining gaps filled with month × hour-of-day\n",
        "#   medians, then overall median fallback.\n",
        "# • Renewable mapping follows the provided grouping. Only fuels present in the dataset are used.\n",
        "# • All outputs are saved under ./data for easy inspection and re-use by a frontend later.\n",
        "# • This notebook is self-contained: can run with API credentials or an offline CSV/Parquet in ./data."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
